---
title: NAACL 22 Tutorial on Human-centered Evaluations of Explanations
layout: page
permalink: /chapter_3.html
---
Ask your questions, and vote on others, on [sli.do](https://app.sli.do/event/awQq8cDeXyxQYFP1WnfGqB).

## Video

## Slides
<iframe src="https://docs.google.com/presentation/d/1tDXNcEu4OZMlsUEMDcJFuH0IYNC8kb3TP-C4YCpb958/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

## Speakers

<div class="col-md-4">
    <div class="profile height150">
        <div><a href="http://www.qveraliao.com"><img class="avatar-img" width=150 src="images/vera.jpg"></a></div>
        <div style="margin-bottom:40px"><center><b>Q. Vera Liao</b><br>Microsoft</center></div>
    </div>
</div>
<div class="col-md-4">
    <div class="profile height150">
        <div><a href="https://alisonmsmith.github.io"><img class="avatar-img" width=150 src="images/alison.jpg"></a></div>
        <div style="margin-bottom:40px"><center><b>Alison Smith-Renner</b><br>Dataminr</center></div>
    </div>
</div>

## References
- **Towards a rigorous science of interpretable machine learning**. *Doshi-Velez, F., & Kim, B.*. 2017. [[paper](https://arxiv.org/abs/1702.08608)]
- **Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems**. *Bu√ßinca, Z., Lin, P., Gajos, K. Z., & Glassman, E. L.* IUI 2020. [[paper](https://arxiv.org/abs/2001.08298)]
- **Human-Centered Explainable AI (XAI): From Algorithms to User Experiences**. *Liao, Q. V., & Varshney, K. R.*. 2021. [[paper](https://arxiv.org/abs/2110.10790)]
- **Questioning the AI: informing design practices for explainable AI user experiences**. *Liao, Q. V., Gruen, D., & Miller, S.* CHI 2020. [[paper](https://arxiv.org/abs/2001.02478)]
- **Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs**. *Suresh, H., Gomez, S. R., Nam, K. K., & Satyanarayan, A.*. CHI 2021. [[paper](https://arxiv.org/abs/2101.09824)]
- **On human predictions with explanations and predictions of machine learning models: A case study on deception detection**. *Lai, V., & Tan, C.* FaCCT 2019. [[paper](https://arxiv.org/abs/1811.07901)]
- **Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making**. *Zhang, Y., Liao, Q. V., & Bellamy, R. K.*. FaCCT 2020. [[paper](https://arxiv.org/abs/2001.02114)]
- **Are explanations helpful? a comparative study of the effects of explanations in AI-assisted decision-making**. *Wang, X., & Yin, M.*.  IUI 2021. [[paper](https://dl.acm.org/doi/10.1145/3397481.3450650)]
- **Developing and validating trust measures for e-commerce: An integrative typology**. *McKnight, D. H., Choudhury, V., & Kacmar, C.*. Information systems research 2002. [[paper](https://pubsonline.informs.org/doi/10.1287/isre.13.3.334.81)]
- **Explaining decision-making algorithms through UI: Strategies to help non-expert stakeholders**. *Cheng, H. F., Wang, R., Zhang, Z., O'Connell, F., Gray, T., Harper, F. M., & Zhu, H.*. CHI 2019. [[paper](https://www.cs.rochester.edu/u/zzhang95/doc/pub/algorithm_explanation_nonstakeholder.pdf)]
- **Towards a Science of Human-AI Decision Making: A Survey of Empirical Studies**. *Lai, V., Chen, C., Liao, Q. V., Smith-Renner, A., & Tan, C.*. 2021. [[paper](https://arxiv.org/abs/2112.11471)]
- ** Interpreting interpretability: understanding data scientists' use of interpretability tools for machine learning**. *Kaur, H., Nori, H., Jenkins, S., Caruana, R., Wallach, H., & Wortman Vaughan, J.*. CHI 2020. [[paper](http://www-personal.umich.edu/~harmank/Papers/CHI2020_Interpretability.pdf)]
- **Explaining models: an empirical study of how explanations impact fairness judgment**. *Dodge, J., Liao, Q. V., Zhang, Y., Bellamy, R. K., & Dugan, C.*. IUI 2019. [[paper](https://arxiv.org/abs/1901.07694)]
