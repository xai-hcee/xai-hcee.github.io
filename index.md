---
title: NAACL 22 Tutorial on Human-centered Evaluations of Explanations
layout: page
permalink: /
---

## Ask/Vote on Questions

Ask your questions, and vote on others, on [sli.do](https://app.sli.do/event/awQq8cDeXyxQYFP1WnfGqB).

## Overview

Recent studies show that many NLP systems are sensitive and vulnerable to a small perturbation of inputs and do not generalize well across different datasets. This lack of robustness derails the use of NLP systems in real-world applications.
This tutorial aims at bringing awareness of practical concerns about NLP robustness. 
It targets NLP researchers and practitioners who are interested in building reliable NLP systems. 
In particular, we will review recent studies on analyzing the weakness of NLP systems when facing adversarial inputs 
and data with a distribution shift. We will provide the audience with a holistic view of
1. how to use adversarial examples to examine the weakness of NLP models and facilitate debugging; 
1. how to enhance the robustness of existing NLP models and defense against adversarial inputs; 
1. how the consideration of robustness affects the real-world NLP applications used in our daily lives. 

We will conclude the tutorial by outlining future research directions in this area.

## Videos

## Slides

## Speakers
<div class="col-md-3">
    <div class="profile height150">
        <div><a href="http://kwchang.net"><img class="avatar-img" width=150 src="https://avatars2.githubusercontent.com/kaiweichang?v=3&s=400"></a></div>
        <div style="margin-bottom:40px"><center><b>Kai-Wei Chang</b><br> UCLA</center></div>
    </div>
</div>

<div class="col-md-3">
    <div class="profile height150">
        <div><a href="https://hhexiy.github.io/"><img class="avatar-img" width=150 src="images/he.png"> </a></div>
        <div style="margin-bottom:40px"><center><b> He He</b> <br> NYU </center></div>
    </div>
</div>
<div class="col-md-3">
    <div class="profile height150">
        <div><a href="https://robinjia.github.io/"><img class="avatar-img" width=150 src="https://avatars2.githubusercontent.com/robinjia?v=3&s=400"></a></div>
        <div style="margin-bottom:40px"><center><b>Robin Jia</b><br> USC</center></div>
    </div>
</div>

<div class="col-md-3">
    <div class="profile height150">
        <div><a href="https://sameersingh.org/"><img class="avatar-img" width=150 src="images/sameer.png"></a></div>
        <div style="margin-bottom:40px"><center><b>Sameer Singh</b><br> UC Irvine</center></div>
    </div>
</div>

## Outline

- Motivation and Overview 
- Finding Lack of Robustness (Attacks) 
    1. Writing Challenging Examples
    1. Generating Adversarial Examples
    1. Adversarial Trigger and Text Generation 
    1. Training Time Attack
- Making Models Robust (Defenses)
    1. Robustness to Spurious Correlations
    1. Adversarial Training for Defense
    1. Certified Robustness in NLP
    1. Test time-defense: detecting adversarial examples
- Conclusion, Future Directions, and Discussion 


## Slides
<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQObhZjgRpHPVStVU2V87P-E4LgsD764B2bY4CUOhOEhORPMXQOnKpmxmtoePFvBW81NDrCn3VaOAT8/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
